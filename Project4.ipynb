{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project 4: Decision Trees\n",
    "\n",
    "### Name: David Hill\n",
    "### Course Level: CSC 448"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Introduction:**\n",
    "* In this project, we explore the application of classification using: Decision Tree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Objectives:**\n",
    "* The objective of this project is to develop software modules for classification of Titanic passengers via decision trees"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## All Students"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* The first problem we aim to us Information Gain to build a decision tree for classification.  The tree will complete when one of two conditions are met:\n",
    "\n",
    "1. The maximum tree depth is reached.\n",
    "2. The minimum number of samples is reached in a given leaf node."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Problem A (100pts)**\n",
    "\n",
    "1 (5pts). The first thing you'll need to do is read the Titatinic dataset from D2L (details about the dataset can be found [Here](https://www.kaggle.com/c/titanic/data?select=test.csv)).  Note that you will need both the train.csv (to build your tree) and test.csv to evaluate the classificaiton accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in the data (however you prefer) #\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# read in training data\n",
    "df = pd.read_csv('train.csv')\n",
    "d_data = df.drop(['PassengerId', 'Survived', 'Name', 'SibSp', 'Ticket', 'Fare', 'Cabin', 'Embarked'], axis=1) # throw out irrelevant columns\n",
    "d_data['Sex'] = d_data['Sex'].map({'female': 0, 'male': 1}) # make sex column numeric values\n",
    "d_data.fillna(d_data.median(), inplace=True) # fill missing values with median\n",
    "d_data = d_data.values # convert to numpy arrays\n",
    "l_labels = df['Survived'].values # only take survived column for labels\n",
    "\n",
    "# read in testing data\n",
    "test_df = pd.read_csv('test.csv')\n",
    "test_data = test_df.drop(['PassengerId', 'Survived', 'Name', 'SibSp', 'Ticket', 'Fare', 'Cabin', 'Embarked'], axis=1)\n",
    "test_data['Sex'] = test_data['Sex'].map({'female': 0, 'male': 1})\n",
    "test_data.fillna(test_data.median(), inplace=True)\n",
    "test_data = test_data.values\n",
    "test_labels = test_df['Survived'].values\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2 (30pts). Next, we need a couple helper functions to compute the conditional entropy and information gain.\n",
    "\n",
    "**Recall: Entropy** \n",
    "$$\n",
    "    H(X) = -\\sum_{\\textbf{x} \\in X} p(\\textbf{x}) \\log_2( p(\\textbf{x}) )\n",
    "$$\n",
    "\n",
    "**Recall: Conditional Entropy** \n",
    "$$\n",
    "    H(Y|X=\\textbf{x}) = -\\sum_{\\textbf{y} \\in Y} p(\\textbf{y}|\\textbf{x}) \\log_2 ( p(\\textbf{y}|\\textbf{x}) )\n",
    "$$\n",
    "where\n",
    "$$\n",
    "    p(\\textbf{y}|\\textbf{x}) = \\frac{p(\\textbf{y},\\textbf{x})}{p(\\textbf{x})}\n",
    "$$\n",
    "and\n",
    "$$\n",
    "    p(\\textbf{x}) = \\sum_{\\textbf{y} \\in Y} p(\\textbf{y},\\textbf{x})\n",
    "$$\n",
    "\n",
    "**Recall: Information Gain** \n",
    "$$\n",
    "    IG(Y|X) = H(Y) - H(Y|X)\n",
    "$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Function definitions here (you may want to modify the input parameters depending on your implimentation #\n",
    "def Entropy(labels):\n",
    "    unique_labels, label_counts = np.unique(labels, return_counts=True)\n",
    "    probabilities = label_counts / len(labels)\n",
    "    H = -np.sum(probabilities * np.log2(probabilities))\n",
    "    return H\n",
    "\n",
    "def C_Entropy(feature_values, labels):\n",
    "    cH = 0\n",
    "    unique_values, label_counts = np.unique(feature_values, return_counts=True)\n",
    "    for value, count in zip(unique_values, label_counts):\n",
    "        probabilities = np.bincount(labels[feature_values == value]) / count # p(y|x)\n",
    "        probabilities = np.clip(probabilities, 1e-15, 1.0 - 1e-15)  # avoid zero probability error\n",
    "        cH += (count / len(feature_values)) * -np.sum(probabilities * np.log2(probabilities))\n",
    "    return cH\n",
    "\n",
    "def IG(labels, feature_values, left_indices, right_indices):\n",
    "    H = Entropy(labels)\n",
    "    cH = 0\n",
    "    # calculate weighted average\n",
    "    for indices in (left_indices, right_indices):\n",
    "        if len(indices) == 0:\n",
    "            continue\n",
    "        cH += (len(indices) / len(labels)) * C_Entropy(feature_values[indices], labels[indices])\n",
    "    IG = H - cH\n",
    "    return IG"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3 (40pts). Define a function to build the tree, recall there should be one of two exit criterion:\n",
    "\n",
    "1. The maximum tree depth is reached\n",
    "2. The minimum number of samples is reached in a given leaf node.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build Tree (can be recursive) #\n",
    "# using OOP\n",
    "class TreeNode:\n",
    "    def __init__(self, predicted_class):\n",
    "        self.predicted_class = predicted_class\n",
    "        self.feature_index = 0\n",
    "        self.threshold = 0\n",
    "        self.left = None\n",
    "        self.right = None\n",
    "\n",
    "def BuildTree(d_data, l_labels, max_depth=None, min_samples_leaf=1, depth=0):\n",
    "    # check if all labels are the same or if max depth is reached or if the data shrinks underneath the minimum samples per leaf\n",
    "    if len(np.unique(l_labels)) == 1 or (max_depth is not None and depth == max_depth) or len(d_data) < min_samples_leaf:\n",
    "        return TreeNode(predicted_class=np.argmax(np.bincount(l_labels)))\n",
    "    \n",
    "    best_feature = None\n",
    "    best_threshold = None\n",
    "    best_IG = -float('inf') # initialize to -infinity\n",
    "\n",
    "    for feature_index in range(d_data.shape[1]):\n",
    "        thresholds = np.unique(d_data[:, feature_index])\n",
    "        for threshold in thresholds:\n",
    "            # get indices based on threshold\n",
    "            left_indices = np.where(d_data[:, feature_index] <= threshold)[0]\n",
    "            right_indices = np.where(d_data[:, feature_index] > threshold)[0]\n",
    "            # prevent split depending on samples parameter\n",
    "            if len(left_indices) < min_samples_leaf or len(right_indices) < min_samples_leaf:\n",
    "                continue\n",
    "            ig = IG(l_labels, d_data[:, feature_index], left_indices, right_indices)\n",
    "            # change \"best\" variables according to IG\n",
    "            if ig > best_IG:\n",
    "                best_IG = ig\n",
    "                best_feature = feature_index\n",
    "                best_threshold = threshold\n",
    "    \n",
    "    # if best_IG didn't change from initialization\n",
    "    if best_IG == -float('inf'):\n",
    "        return TreeNode(predicted_class=np.argmax(np.bincount(l_labels)))\n",
    "\n",
    "    # get indices based on BEST threshold now\n",
    "    left_indices = np.where(d_data[:, best_feature] <= best_threshold)[0]\n",
    "    right_indices = np.where(d_data[:, best_feature] > best_threshold)[0]\n",
    "\n",
    "    # build left and right trees recursively and increase depth\n",
    "    left_tree = BuildTree(d_data[left_indices], l_labels[left_indices], max_depth, min_samples_leaf, depth + 1)\n",
    "    right_tree = BuildTree(d_data[right_indices], l_labels[right_indices], max_depth, min_samples_leaf, depth + 1)\n",
    "\n",
    "    # get node values and return it\n",
    "    node = TreeNode(predicted_class=None)\n",
    "    node.feature_index = best_feature\n",
    "    node.threshold = best_threshold\n",
    "    node.left = left_tree\n",
    "    node.right = right_tree\n",
    "\n",
    "    return node"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4 (20pts). Next write a function to perform the prediciton (classificaiton) of a new training sample:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to predict #\n",
    "def DT_predict(d_tree,d_sample):\n",
    "    # traverse the tree until a leaf node is reached\n",
    "    while d_tree.predicted_class is None:\n",
    "        if d_sample[d_tree.feature_index] <= d_tree.threshold:\n",
    "            d_tree = d_tree.left\n",
    "        else:\n",
    "            d_tree = d_tree.right\n",
    "    return d_tree.predicted_class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5 (5pts). Using the test set, evaluate the accuracy of your classification tree."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7679425837320574\n"
     ]
    }
   ],
   "source": [
    "# Classification Accuracy #\n",
    "# using test data and test labels read in above\n",
    "\n",
    "# build tree\n",
    "tree = BuildTree(d_data, l_labels, max_depth=3, min_samples_leaf=10)\n",
    "\n",
    "# calculate correct predictions out of length of the test data\n",
    "correct_predictions = 0\n",
    "for i in range(len(test_data)):\n",
    "    sample = test_data[i]\n",
    "    actual_result = test_labels[i]\n",
    "    prediction = DT_predict(tree, sample)\n",
    "    if prediction == actual_result:\n",
    "        correct_predictions += 1\n",
    "acc = correct_predictions / len(test_data)\n",
    "\n",
    "print(\"Accuracy:\", acc)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
